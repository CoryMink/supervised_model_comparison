{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Supervised Learning Model Comparison\n",
    "\n",
    "_author The arbitrary and capricious heart of data science_\n",
    "\n",
    "---\n",
    "\n",
    "### Let us begin...\n",
    "\n",
    "Recall the \"data science process.\"\n",
    "   1. Define the problem.\n",
    "   2. Gather the data.\n",
    "   3. Explore the data.\n",
    "   4. Model the data.\n",
    "   5. Evaluate the model.\n",
    "   6. Answer the problem.\n",
    "\n",
    "In this lab, we're going to focus mostly on creating (and then comparing) many regression and classification models. Thus, we'll define the problem and gather the data for you.\n",
    "Most of the questions requiring a written response can be written in 2-3 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the problem.\n",
    "\n",
    "You are a data scientist with a financial services company. Specifically, you want to leverage data in order to identify potential customers.\n",
    "\n",
    "If you are unfamiliar with \"401(k)s\" or \"IRAs,\" these are two types of retirement accounts. Very broadly speaking:\n",
    "- You can put money for retirement into both of these accounts.\n",
    "- The money in these accounts gets invested and hopefully has a lot more money in it when you retire.\n",
    "- These are a little different from regular bank accounts in that there are certain tax benefits to these accounts. Also, employers frequently match money that you put into a 401k.\n",
    "- If you want to learn more about them, check out [this site](https://www.nerdwallet.com/article/ira-vs-401k-retirement-accounts).\n",
    "\n",
    "We will tackle one regression problem and one classification problem today.\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether or not one is eligible for a 401k.\n",
    "\n",
    "Check out the data dictionary [here](http://fmwww.bc.edu/ec-p/data/wooldridge2k/401KSUBS.DES).\n",
    "\n",
    "### NOTE: When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable. When predicting `e401k`, you may use the entire dataframe if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas                as pd\n",
    "import numpy                 as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.linear_model    import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors       import KNeighborsRegressor\n",
    "from sklearn.tree            import DecisionTreeRegressor\n",
    "from sklearn.ensemble        import BaggingRegressor, RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.metrics         import mean_squared_error, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Gather the data.\n",
    "\n",
    "##### 1. Read in the data from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('401ksubs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. What are 2-3 other variables that, if available, would be helpful to have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "1) Investment of any forms --> might decrease amount in 401k(s) or IRA deposit \\\n",
    "2) Indexed Universal Life Insurance --> some people choose this over 401k/IRA since, it could deduct tax too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Suppose a peer recommended putting `race` into your model in order to better predict who to target when advertising IRAs and 401(k)s. Why would this be an unethical decision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Race data might raise biased-conclusion or offend people in some way if wording is not properly use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Explore the data.\n",
    "\n",
    "##### 4. When attempting to predict income, which feature(s) would we reasonably not use? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Family size and marriage might not tell us much about income. \\\n",
    "In the other hand, outcome will be related to family size and marriage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. What two variables have already been created for us through feature engineering? Come up with a hypothesis as to why subject-matter experts may have done this.\n",
    "> This need not be a \"statistical hypothesis.\" Just brainstorm why SMEs might have done this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "There are age-square and inc-square that has been pre-made in the dataset which means SMEs evaluate these two variable as a key variable and weighted them more than other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Looking at the data dictionary, one variable description appears to be an error. What is this error, and what do you think the correct value would be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Actually, I noticed two wrong variable-descriptions which are \\\n",
    "age = age^2 and inc = inc^2, but the values are fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data. (Part 1: Regression Problem)\n",
    "\n",
    "Recall:\n",
    "- Problem: What features best predict one's income?\n",
    "- When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable.\n",
    "\n",
    "##### 7. List all modeling tactics we've learned that could be used to solve a regression problem (as of Wednesday afternoon of Week 6). For each tactic, identify whether it is or is not appropriate for solving this specific regression problem and explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "| Regression | Details |\n",
    "| --- | --- |\n",
    "| Linear Regression | Easy to interpret, explain, and evaluate coefficients |\n",
    "| Ridge Regression | Improve predictive power by dealing with multicollinearity |\n",
    "| Lasso Regression | More aggressive than Ridge Regression |\n",
    "| ElasticNet Regression | Combination of Lasso & Ridge Regression |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Regardless of your answer to number 7, fit at least one of each of the following models to attempt to solve the regression problem above:\n",
    "    - a multiple linear regression model\n",
    "    - a k-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend setting a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['e401k', 'inc', 'marr', 'male', 'age', 'fsize', 'nettfa', 'p401k',\n",
       "       'pira', 'incsq', 'agesq'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for features\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set X and y --> train_test_split --> Scale --> Fitting --> Cross Validation Score evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostRegressor</label><div class=\"sk-toggleable__content\"><pre>AdaBoostRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "AdaBoostRegressor()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set X and y\n",
    "X = df[['marr', 'male', 'fsize', 'nettfa', 'agesq']]\n",
    "y = df['incsq']\n",
    "\n",
    "# Train test split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=42)\n",
    "\n",
    "# Scale X_train and X_test\n",
    "ss = StandardScaler()\n",
    "X_train_sc = ss.fit_transform(X_train)\n",
    "X_test_sc = ss.transform(X_test)\n",
    "\n",
    "# Initiation\n",
    "lr = LinearRegression()\n",
    "knn= KNeighborsRegressor()\n",
    "dt = DecisionTreeRegressor()\n",
    "bt = BaggingRegressor()\n",
    "rf = RandomForestRegressor()\n",
    "ad = AdaBoostRegressor()\n",
    "\n",
    "# Fitting\n",
    "lr.fit(X_train_sc, y_train)\n",
    "knn.fit(X_train_sc, y_train)\n",
    "dt.fit(X_train, y_train)\n",
    "bt.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "ad.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of Linear Regression:     0.24783806903154507\n",
      "Score of kNN Regression:        0.21144319227510655\n",
      "Score of Decision Trees:       -0.24837547506006383\n",
      "Score of Bagged Decision Trees: 0.20115907813635459\n",
      "Score of Random Forest:         0.23963076605358635\n",
      "Score of Adaboost:             -0.2737728166661988\n"
     ]
    }
   ],
   "source": [
    "# Check for Cross validation score\n",
    "print(f'Score of Linear Regression:     {cross_val_score(lr, X_train_sc, y_train).mean()}')\n",
    "print(f'Score of kNN Regression:        {cross_val_score(knn, X_train_sc, y_train).mean()}')\n",
    "print(f'Score of Decision Trees:       {cross_val_score(dt, X_train, y_train).mean()}')\n",
    "print(f'Score of Bagged Decision Trees: {cross_val_score(bt, X_train, y_train).mean()}')\n",
    "print(f'Score of Random Forest:         {cross_val_score(rf, X_train, y_train).mean()}')\n",
    "print(f'Score of Adaboost:             {cross_val_score(ad, X_train, y_train).mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. What is bootstrapping?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "In machine learning, bootstrapping involves repeatedly drawing samples from our source data with replacement.\n",
    "Same rows, but resampled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. What is the difference between a decision tree and a set of bagged decision trees? Be specific and precise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Bagged decision trees = Decision trees + Bootstrapping , while Decision trees doesn't has bootstrapping. \\\n",
    "What is bootstrapping? Read the thread above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11. What is the difference between a set of bagged decision trees and a random forest? Be specific and precise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Random forests differ from bagged trees by forcing the tree to use only a subset of its available predictors to split on in the growing phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12. Why might a random forest be superior to a set of bagged decision trees?\n",
    "> Hint: Consider the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "<u>Evolution of Random Forest: Decision Trees$^{*}$ --> Bagged Decision Trees$^{**}$ --> Random Forest$^{***}$</u> \\\n",
    "Random forest is the enhanced version of Bagged Decision Trees model which is essentially an ensemble of decision trees trained with a bagging mechanism.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model. (Part 1: Regression Problem)\n",
    "\n",
    "##### 13. Using RMSE, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Train Score:     2577.458304052816\n",
      "Linear Regression Test Score:      2771.707049080118\n",
      "\n",
      "kNN Train Score:                   2149.5263402618393\n",
      "kNN Test Score:                    2696.482628491796\n",
      "\n",
      "Decision Trees Train Score:        3392.8471168314013\n",
      "Decision Trees Test Score:         3438.9915313023803\n",
      "\n",
      "Bagged Decision Trees Train Score: 3056.308784682495\n",
      "Bagged Decision Trees Test Score:  3123.889431801379\n",
      "\n",
      "Random Forest Train Score:         3039.0477405417614\n",
      "Random Forest Test Score:          3110.5548520442835\n",
      "\n",
      "Adaboost Train Score:              2821.690140101233\n",
      "Adaboost Test Score:               2909.166356005474\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Predict on X_train and X_test for RMSE score\n",
    "y_lrtr= lr.predict(X_train_sc)\n",
    "print(f'Linear Regression Train Score:     {mean_squared_error(y_train, y_lrtr, squared=False)}')\n",
    "y_lrtt= lr.predict(X_test_sc)\n",
    "print(f'Linear Regression Test Score:      {mean_squared_error(y_test, y_lrtt, squared=False)}')\n",
    "print('') # =======================================================================================#\n",
    "y_knntr= knn.predict(X_train_sc)\n",
    "print(f'kNN Train Score:                   {mean_squared_error(y_train, y_knntr, squared=False)}')\n",
    "y_knntt= knn.predict(X_test_sc) \n",
    "print(f'kNN Test Score:                    {mean_squared_error(y_test, y_knntt, squared=False)}')\n",
    "print('') # =======================================================================================#\n",
    "y_dttr= dt.predict(X_train_sc)\n",
    "print(f'Decision Trees Train Score:        {mean_squared_error(y_train, y_dttr, squared=False)}')\n",
    "y_dttt= dt.predict(X_test_sc)\n",
    "print(f'Decision Trees Test Score:         {mean_squared_error(y_test, y_dttt, squared=False)}')\n",
    "print('') # =======================================================================================#\n",
    "y_bttr= bt.predict(X_train_sc)\n",
    "print(f'Bagged Decision Trees Train Score: {mean_squared_error(y_train, y_bttr, squared=False)}')\n",
    "y_bttt= bt.predict(X_test_sc)\n",
    "print(f'Bagged Decision Trees Test Score:  {mean_squared_error(y_test, y_bttt, squared=False)}')\n",
    "print('') # =======================================================================================#\n",
    "y_rftr= rf.predict(X_train_sc)\n",
    "print(f'Random Forest Train Score:         {mean_squared_error(y_train, y_rftr, squared=False)}')\n",
    "y_rftt= rf.predict(X_test_sc)\n",
    "print(f'Random Forest Test Score:          {mean_squared_error(y_test, y_rftt, squared=False)}')\n",
    "print('') # =======================================================================================#\n",
    "y_adtr= ad.predict(X_train_sc)\n",
    "print(f'Adaboost Train Score:              {mean_squared_error(y_train, y_adtr, squared=False)}')\n",
    "y_adtt= ad.predict(X_test_sc)\n",
    "print(f'Adaboost Test Score:               {mean_squared_error(y_test, y_adtt, squared=False)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 14. Based on training RMSE and testing RMSE, is there evidence of overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Decision Trees, Bagged Decision Trees, Random Forest, and Adaboost shown overfitting \\\n",
    "(Test Score < Train Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 15. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "I would pick Linear Regression model since, there is no sign of overfitting, trani-test score difference is not high as kNN score, and most of all, it is easy to explain to most non-technical audience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 16. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "1) GridSearch \\\n",
    "2) Try polynomial features on dataset other than squareroot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data. (Part 2: Classification Problem)\n",
    "\n",
    "Recall:\n",
    "- Problem: Predict whether or not one is eligible for a 401k.\n",
    "- When predicting `e401k`, you may use the entire dataframe if you wish.\n",
    "\n",
    "##### 17. While you're allowed to use every variable in your dataframe, mention at least one disadvantage of using `p401k` in your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "A person in 'p401k' column is the person who has invested in 401k which will also be 'True' or '1' in e401k column. \\\n",
    "It is almost the same with including target variable into predictor variable which will not lead to great predictive result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 18. List all modeling tactics we've learned that could be used to solve a classification problem (as of Wednesday afternoon of Week 6). For each tactic, identify whether it is or is not appropriate for solving this specific classification problem and explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "-  Logistic Regression:  An appropriate tactic, especially since its coefficients can be interpreted.\n",
    "-  KNearest Neighbors:  An appropriate tactic, as it can be used for Classification purposes.\n",
    "-  Decision Trees:  An appropriate tactic, as it can be used for Classification purposes.\n",
    "-  Bagged Decision Trees:  An appropriate tactic, as it can be used for Classification purposes.\n",
    "-  Random Forest:  An appropriate tactic, as it can be used for Classification purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19. Regardless of your answer to number 18, fit at least one of each of the following models to attempt to solve the classification problem above:\n",
    "    - a logistic regression model\n",
    "    - a k-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend using a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['e401k', 'inc', 'marr', 'male', 'age', 'fsize', 'nettfa', 'p401k',\n",
       "       'pira', 'incsq', 'agesq'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for features\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-13 {color: black;background-color: white;}#sk-container-id-13 pre{padding: 0;}#sk-container-id-13 div.sk-toggleable {background-color: white;}#sk-container-id-13 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-13 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-13 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-13 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-13 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-13 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-13 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-13 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-13 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-13 div.sk-item {position: relative;z-index: 1;}#sk-container-id-13 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-13 div.sk-item::before, #sk-container-id-13 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-13 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-13 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-13 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-13 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-13 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-13 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-13 div.sk-label-container {text-align: center;}#sk-container-id-13 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-13 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-13\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" checked><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostRegressor</label><div class=\"sk-toggleable__content\"><pre>AdaBoostRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "AdaBoostRegressor()"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set X and y\n",
    "X = df[['marr', 'male', 'age', 'fsize', 'nettfa', 'pira', 'incsq', 'agesq']]\n",
    "y = df['e401k']\n",
    "\n",
    "# Train test split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=42)\n",
    "\n",
    "# Scale X_train and X_test\n",
    "ss = StandardScaler()\n",
    "X_train_sc = ss.fit_transform(X_train)\n",
    "X_test_sc = ss.transform(X_test)\n",
    "\n",
    "# Initiation\n",
    "logr = LogisticRegression()\n",
    "knn  = KNeighborsRegressor()\n",
    "dt   = DecisionTreeRegressor()\n",
    "bt   = BaggingRegressor()\n",
    "rf   = RandomForestRegressor()\n",
    "ad   = AdaBoostRegressor()\n",
    "\n",
    "# Fitting\n",
    "logr.fit(X_train_sc, y_train)\n",
    "knn.fit(X_train_sc, y_train)\n",
    "dt.fit(X_train, y_train)\n",
    "bt.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "ad.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of Logistic Regression:   0.6434732310336564\n",
      "Score of kNN Regression:       -0.041498108325827586\n",
      "Score of Decision Trees:       -0.6907686811561575\n",
      "Score of Bagged Decision Trees: 0.0033380539367735772\n",
      "Score of Random Forest:         0.07533154408726442\n",
      "Score of Adaboost:              0.11666540033396107\n"
     ]
    }
   ],
   "source": [
    "# Check for Cross validation score\n",
    "print(f'Score of Logistic Regression:   {cross_val_score(logr, X_train_sc, y_train).mean()}')\n",
    "print(f'Score of kNN Regression:       {cross_val_score(knn, X_train_sc, y_train).mean()}')\n",
    "print(f'Score of Decision Trees:       {cross_val_score(dt, X_train, y_train).mean()}')\n",
    "print(f'Score of Bagged Decision Trees: {cross_val_score(bt, X_train, y_train).mean()}')\n",
    "print(f'Score of Random Forest:         {cross_val_score(rf, X_train, y_train).mean()}')\n",
    "print(f'Score of Adaboost:              {cross_val_score(ad, X_train, y_train).mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- kNN, Decision Tree, and Bagged Decision trees shown bad fitting. \n",
    "- Logistic Regression is the best choice in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model. (Part 2: Classfication Problem)\n",
    "\n",
    "##### 20. Suppose our \"positive\" class is that someone is eligible for a 401(k). What are our false positives? What are our false negatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "e401(k) == 1 --> TP \\\n",
    "e401(k) == 0 --> TN \\\n",
    "p401(k) == 1 --> FP \\\n",
    "p401(k) == 0 --> FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21. In this specific case, would we rather minimize false positives or minimize false negatives? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Minimizing false-negatives (error Type II) might be better than minimizing faslse-positives (error Type I) in this case.\n",
    "\n",
    "A person with 'p401(k) == 0' doesn't mean that a person is 'e401(k) ==0' or not eligible to invest in 401K.\n",
    "\n",
    "While people who 'p401(k) ==1' are people who eligible to invest in 401K or 'e401(k) == 1' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22. Suppose we wanted to optimize for the answer you provided in problem 21. Which metric would we optimize in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "The false negative rate (miss rate) – is the probability that a true positive will be missed by the test. \n",
    "\n",
    "$FN\\% = \\frac{FN}{FN+TP} $\n",
    "\n",
    "Optimizing TP will reduce the FN% = Optimizing Sensitivity \\\n",
    "could reduce error type II\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23. Suppose that instead of optimizing for the metric in problem 21, we wanted to balance our false positives and false negatives using `f1-score`. Why might [f1-score](https://en.wikipedia.org/wiki/F1_score) be an appropriate metric to use here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "f1 score calculate from True Positive, False Positive, and False Negative according to the equation below: \n",
    "\n",
    "$F_{1}Score = \\frac{2TP}{2TP+FN+FP}$\n",
    "\n",
    "Having equation that contains our interested variables should help balancing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24. Using f1-score, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 LogisticRegression train:    0.3342303552206674\n",
      "F1 LogisticRegression test:     0.3450531479967293\n",
      "\n",
      "F1 kNN Regression train:        0.6562998405103669\n",
      "F1 kNN Regression test:         0.510353227771011\n",
      "\n",
      "F1 Bagged Decision Trees train: 0.4474650991917708\n",
      "F1 Bagged Decision Trees test:  0.43197026022304835\n",
      "\n",
      "F1 Random Forest train:         0.06662087912087912\n",
      "F1 Random Forest test:          0.07135362014690451\n",
      "\n",
      "F1 Adaboost train:              0.006531204644412192\n",
      "F1 Adaboost test:               0.00887902330743618\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict y_train and y_test for F1 score\n",
    "y_logr_train = logr.predict(X_train_sc)\n",
    "y_logr_test = logr.predict(X_test_sc)\n",
    "print(f'F1 LogisticRegression train:    {f1_score(y_train, y_logr_train)}')\n",
    "print(f'F1 LogisticRegression test:     {f1_score(y_test, y_logr_test)}')\n",
    "print('') # =======================================================================================#\n",
    "y_knn_train = knn.predict(X_train_sc)\n",
    "y_knn_test  = knn.predict(X_test_sc)\n",
    "print(f'F1 kNN Regression train:        {f1_score(y_train, y_knn_train.round())}')\n",
    "print(f'F1 kNN Regression test:         {f1_score(y_test, y_knn_test.round())}')\n",
    "print('') # =======================================================================================#\n",
    "y_bt_train = bt.predict(X_train_sc)\n",
    "y_bt_test = bt.predict(X_test_sc)\n",
    "print(f'F1 Bagged Decision Trees train: {f1_score(y_train, y_bt_train.round())}')\n",
    "print(f'F1 Bagged Decision Trees test:  {f1_score(y_test, y_bt_test.round())}')\n",
    "print('') # =======================================================================================#\n",
    "y_rf_train = rf.predict(X_train_sc)\n",
    "y_rf_test = rf.predict(X_test_sc)\n",
    "print(f'F1 Random Forest train:         {f1_score(y_train, y_rf_train.round())}')\n",
    "print(f'F1 Random Forest test:          {f1_score(y_test, y_rf_test.round())}')\n",
    "print('') # =======================================================================================#\n",
    "y_ad_train = ad.predict(X_train_sc)\n",
    "y_ad_test = ad.predict(X_test_sc)\n",
    "print(f'F1 Adaboost train:              {f1_score(y_train, y_ad_train.round())}')\n",
    "print(f'F1 Adaboost test:               {f1_score(y_test, y_ad_test.round())}')\n",
    "print('') # =======================================================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25. Based on training f1-score and testing f1-score, is there evidence of overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "There is only one overfitting model which is K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Based on F1 score, K-nearest Neighbors ,eventhough it is overfitting model, has highest F1 score.\n",
    "\n",
    "F1 = 1 indicating perfect precision and recall, kNN show F1 closest to 1 among others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "- I would love to try Stacking models.\n",
    "- SMOTE might be great too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Answer the problem.\n",
    "\n",
    "##### BONUS: Briefly summarize your answers to the regression and classification problems. Be sure to include any limitations or hesitations in your answer.\n",
    "\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether or not one is eligible for a 401k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "My final answers for this case are:\n",
    "- Regression: Linear Regression\n",
    "- Classification: K-nearest neighbors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
